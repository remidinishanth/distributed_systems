The transformer uses self-attention mechanisms to focus on different parts of the input text during each processing step. 
This allows the model to consider the context of each word in relation to other words in the input text.

<img width="1520" alt="image" src="https://github.com/user-attachments/assets/a39a7017-4c2a-471d-9bba-a90fef792252">


Refer:
* https://www.youtube.com/watch?v=SZorAJ4I-sA&ab_channel=GoogleCloudTech

  - 🚀 Transformers revolutionize machine learning, enabling tasks like translation and code generation.
  - 🔍 BERT, GPT-3, and T5 are all transformer-based models reshaping natural language processing.
  - 🧠 Positional encodings help capture word order, improving language understanding.
  - ⚡ Transformers allow parallel processing, making them faster to train than RNNs.
  - 📊 Self-attention enables context-aware understanding, enhancing meaning recognition in language.
  - 🌐 BERT utilizes semi-supervised learning for better performance on unlabeled data.

* Attention is all you need https://arxiv.org/abs/1706.03762
* [AWESOME] https://www.youtube.com/watch?v=Pnd8bCJ4Z3A
